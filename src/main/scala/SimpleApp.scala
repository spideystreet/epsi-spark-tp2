import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.col // On importe la fonction 'col'

object SimpleApp {
  def main(args: Array[String]): Unit = {
    
    val spark = SparkSession.builder
      .appName("TP Spark - Partie 1")
      .config("spark.hadoop.user.name", "hadoop")
      .master("local[*]") // Utilisation du master local
      .getOrCreate()

    // --- 1. Chargement des donnÃ©es ---
    println("1. Chargement du fichier train_clean.csv...")
    val df = spark.read
      .option("header", "true") // Utilise la premiÃ¨re ligne comme en-tÃªte
      .option("inferSchema", "true") // Spark essaie de deviner les types
      .csv("data/train_clean.csv")

    // --- 2. Affichage des dimensions ---
    val rowCount = df.count()
    val colCount = df.columns.length
    println(s"2. Le DataFrame a $rowCount lignes et $colCount colonnes.")

    // --- 3. Affichage d'un extrait ---
    println("3. Extrait du DataFrame :")
    df.show(5, false) // 5 lignes, sans tronquer les colonnes

    // --- 4. Affichage du schÃ©ma ---
    println("4. SchÃ©ma du DataFrame :")
    df.printSchema()

    // --- 5. Assignation des types Entier ---
    println("5. Conversion des colonnes en Entier...")
    val dfCasted = df
      .withColumn("goal", col("goal").cast("integer"))
      .withColumn("final_status", col("final_status").cast("integer"))
    
    println("SchÃ©ma aprÃ¨s conversion :")
    dfCasted.printSchema()


    // On arrÃªte la session Spark
    spark.stop()
    println("\nðŸ‘‹ Fin de la Partie 1. Session Spark arrÃªtÃ©e. ðŸ‘‹")
  }
}